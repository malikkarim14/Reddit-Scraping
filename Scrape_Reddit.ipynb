{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1UCpHpmXPqYVkNaGO_epaS2rFKgdHTmIV",
      "authorship_tag": "ABX9TyN8sjtqm/SkcEuRC0QkHtsy"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LhoEj6J9cOQ",
        "outputId": "1bc57db9-62c2-4335-f856-b03b2264591a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import requests\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getdata(url):\n",
        "  header = {'User-agent' :'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.114 Safari/537.36 Edg/103.0.1264.62'}\n",
        "  r = requests.get(url, headers = header)\n",
        "  return r.text"
      ],
      "metadata": {
        "id": "QH618eugxHAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://old.reddit.com/r/todayilearned/top/?sort=top&t=all'\n",
        "\n",
        "htmldata = getdata(url)\n",
        "soup = BeautifulSoup(htmldata, 'html.parser')"
      ],
      "metadata": {
        "id": "yM1YOEbKxWHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attrs = {'class': 'thing', 'data-whitelist-status': 'all_ads'}\n",
        "\n",
        "counter = 1\n",
        "\n",
        "while True:\n",
        "  for post in soup.find_all('div', attrs=attrs):\n",
        "    title = post.find('p', class_=\"title\").text\n",
        "    try:\n",
        "      author = post.find('a', class_='author').text\n",
        "    except:\n",
        "      author = \"deleted user\"\n",
        "    post_time = post.find('time').text\n",
        "    comments = post.find('a', class_='comments').text.split()[0]\n",
        "    if comments == \"comment\":\n",
        "      comments = 0\n",
        "    likes = post.find(\"div\", attrs={\"class\": \"score likes\"}).text\n",
        "    source = post.find('a', href=True)\n",
        "    source_link = source['href']\n",
        "    \n",
        "    #Write to csv\n",
        "    post_line = [counter, title, author, post_time, comments, likes, source_link]\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/todayilearned_scrapped_data.csv', 'a') as f:\n",
        "      writer = csv.writer(f)\n",
        "      writer.writerow(post_line)\n",
        "    \n",
        "    counter += 1\n",
        "    if counter > 950:\n",
        "      break\n",
        "  \n",
        "  if counter > 950:\n",
        "    break\n",
        "\n",
        "  #Move to next page\n",
        "  next_button = soup.find(\"span\", class_=\"next-button\")\n",
        "  next_page_link = next_button.find(\"a\").attrs['href']\n",
        "  time.sleep(2)\n",
        "  page = getdata(next_page_link)\n",
        "  soup = BeautifulSoup(page, 'html.parser')"
      ],
      "metadata": {
        "id": "1ubPBeIgqIF7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}